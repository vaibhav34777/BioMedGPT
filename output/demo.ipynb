{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afadca40-ea16-4d66-937b-58b716b21cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imvai\\AppData\\Local\\Temp\\ipykernel_29420\\1866085032.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint_step1050', map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "import time\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import torch.utils\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_embd : int = 1024\n",
    "    n_layer : int =24\n",
    "    block_size : int = 1024\n",
    "    vocab_size : int =50257  # converting to a nice number \n",
    "    n_head : int = 16\n",
    "\n",
    "# ARCHITECTURE OF THE GPT MODEL\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        assert config.n_embd%config.n_head==0\n",
    "        super().__init__()\n",
    "        self.c_attn=nn.Linear(config.n_embd,3*config.n_embd)\n",
    "        #self.register_buffer('tril',torch.tril(torch.ones(config.block_size,config.block_size)).unsqueeze(0).unsqueeze(0))\n",
    "        # for regularising and making the n_head a batch dimension\n",
    "        self.n_head=config.n_head\n",
    "        self.n_embd=config.n_embd\n",
    "        self.c_proj=nn.Linear(config.n_embd,config.n_embd)\n",
    "        self.c_proj.NANO_GPT=1\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C=x.shape\n",
    "        qkv=self.c_attn(x)\n",
    "        q,k,v=qkv.split(self.n_embd,dim=2)\n",
    "        q=q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)  # (B,N_HEAD,T,C)\n",
    "        k=k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)  # (B,N_HEAD,T,C)\n",
    "        v=v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)  # (B,N_HEAD,T,C)\n",
    "        # wei=q @ k.transpose(2,3)   # (B,N_HEAD,T.T)\n",
    "        # wei=wei.masked_fill(self.tril[:,:,:T,:T]==0,float('-inf'))\n",
    "        # wei=F.softmax(wei,dim=-1)\n",
    "        # y=wei @ v  # (B,N_HEAD,T,C)\n",
    "        y=F.scaled_dot_product_attention(q,k,v,is_causal=True) # flash attention\n",
    "        y=y.transpose(1,2).contiguous().view(B,T,C)  # (B,T,C)\n",
    "        y=self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.n_embd,4*config.n_embd)\n",
    "        self.gelu=nn.GELU(approximate='tanh')\n",
    "        self.c_proj=nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        self.c_proj.NANO_GPT=1\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.c_fc(x)\n",
    "        x=self.gelu(x)\n",
    "        x=self.c_proj(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.n_embd)\n",
    "        self.attn=CausalSelfAttention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.n_embd)\n",
    "        self.mlp=MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.transformer= nn.ModuleDict(dict(\n",
    "            wte =nn.Embedding(config.vocab_size,config.n_embd),\n",
    "            wpe =nn.Embedding(config.block_size,config.n_embd),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head=nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.transformer.wte.weight          # weight sharing\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            std=0.02\n",
    "            if hasattr(module,'NANO_GPT'):\n",
    "                std*=(2*self.config.n_layer)**-0.5\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T=idx.shape\n",
    "        assert T<=self.config.block_size\n",
    "        tok_emb=self.transformer.wte(idx)\n",
    "        pos=torch.arange(0,T,dtype=torch.long,device=idx.device)\n",
    "        pos_emb=self.transformer.wpe(pos)\n",
    "        x=tok_emb + pos_emb\n",
    "        # forward pass\n",
    "        for block in self.transformer.h:\n",
    "            x=block(x)\n",
    "        x=self.transformer.ln_f(x)\n",
    "        logits=self.lm_head(x)\n",
    "        loss=None\n",
    "        if targets is not None:\n",
    "            loss=F.cross_entropy(logits.view(-1,self.config.vocab_size),targets.view(-1))\n",
    "        return logits,loss\n",
    "\n",
    "# LOAD THE MODEL STATE DICT IN THIS GPT ARCHITECTURE\n",
    "\n",
    "checkpoint = torch.load('checkpoint_step1050', map_location='cpu')\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in checkpoint.items()}\n",
    "model=GPT(GPTConfig())\n",
    "model.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd65d714-bda1-478a-818d-53f13089169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does CRISPR-Cas9 technology enable genome editing\n",
      "\n",
      "\n",
      "Answer: CRISPR-Cas9 provides the ability to target specific regions of a genome from a patient's immune cells and deliver targeted DNA to cells outside the body. The development of new therapies targeting specific regions of cells leads to the development of novel therapies, which in turn leads to new therapies. In addition, CRISPR-Cas9 creates functional immune cells that can recognize and respond to different types of pathogens, which in turn leads to novel therapeutics.\n",
      "\n",
      "How does CRISPR-Cas9 help treat patients with autoimmune diseases\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "num_sequence=1\n",
    "max_tokens=200\n",
    "enc=tiktoken.get_encoding('gpt2')\n",
    "tokens=enc.encode(\"Question: How does CRISPR-Cas9 technology enable genome editing?\")\n",
    "tokens=torch.tensor(tokens,dtype=torch.long)\n",
    "tokens=tokens.unsqueeze(0).repeat(num_sequence,1)\n",
    "# predicting the next token\n",
    "while tokens.shape[1]<max_tokens:\n",
    "    with torch.no_grad():\n",
    "        logits,loss=model(tokens)\n",
    "        logits=logits[:,-1,:]\n",
    "        probs=F.softmax(logits,dim=-1) \n",
    "        topk_probs,topk_indices=torch.topk(probs,50,dim=-1)  # storing top 50 probs and their indexes for the batch\n",
    "        idx=torch.multinomial(topk_probs,num_samples=1)        # will return a index b/w 0 to 50\n",
    "        xcol=torch.gather(topk_indices,-1,idx)  # gathering the original indexes using predicted idx and return a tensor of next indexes\n",
    "        if xcol[-1]==50256:\n",
    "            break\n",
    "        tokens=torch.cat((tokens,xcol),dim=1)\n",
    "# decoding\n",
    "for i in range(num_sequence):\n",
    "    x=tokens[i,:max_tokens].tolist()\n",
    "    decoded=enc.decode(x)\n",
    "    decode=decoded.split('?')\n",
    "    print(decode[0])\n",
    "    print(decode[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cac391-69e3-4836-99e2-1d1379cf8ee2",
   "metadata": {},
   "source": [
    "### Q&A Pairs for predicted and expected answers \n",
    "#### Question: How does the renin-angiotensin-aldosterone system (RAAS) regulate blood pressure, and what are the implications of ACE inhibitors in its modulation?\n",
    "#### Answer: \n",
    "It is largely a system-dependent mechanism. Activation of the renin-angiotensin-aldosterone system in the presence of arterial occlusion results in increases in systolic and diastolic blood pressures. In hypertensive patients, the RAAS activity is increased to a greater extent than in normotensive patients. The increased activity of the system may contribute to the increased vascular resistance of patients with hypertension.In response to vasodilation, the system may decrease, and in some patients can increase dangerously as a result of the increase in plasma endothelial nitric oxide (VENO). The increased endothelial nitric oxide may contribute to increases in the release of proinflammatory cytokines.\n",
    "#### Expected Answer: \n",
    "The renin-angiotensin-aldosterone system (RAAS) plays a pivotal role in regulating blood pressure and fluid balance. When blood pressure falls, the kidneys release renin, which converts angiotensinogen from the liver into angiotensin I. ACE (angiotensin-converting enzyme) in the lungs then converts angiotensin I into angiotensin II, a potent vasoconstrictor that increases blood pressure. Additionally, angiotensin II stimulates the adrenal cortex to secrete aldosterone, which promotes sodium and water retention, further raising blood pressure.ACE inhibitors block the conversion of angiotensin I to angiotensin II, leading to reduced vasoconstriction and aldosterone release. This results in lower blood pressure and decreased stress on the cardiovascular system, making ACE inhibitors a critical treatment option for hypertension, heart failure, and certain kidney diseases.\n",
    "\n",
    "#### Question: What role do cytochrome P450 enzymes play in drug metabolism?\n",
    "#### Answer:\n",
    "CYP2D6 enzymes play a role in drug metabolism. CYP4A4 enzymes have a very small effect in drug metabolism, which means that they are not involved in the generation of active metabolites. Acetylation of CYP4A4 enzyme leads to the activation of PPARG and subsequent drug production, which has been shown to increase the effectiveness of drug therapy.\n",
    "The mechanisms of CYP2D6/P4501A4 activation are not well understood, and it is currently unclear whether activation of PPARG leads to a more efficient metabolism of drugs.\n",
    "#### Expected Answer: \n",
    "Cytochrome P450 enzymes are a family of heme-containing enzymes that play a crucial role in the metabolism of many drugs. They function primarily by catalyzing oxidation reactions, converting lipophilic compounds into more water-soluble metabolites that can be more easily excreted from the body. Key isoforms, such as CYP3A4, CYP2D6, and CYP2C9, are responsible for metabolizing a large proportion of pharmaceuticals. This metabolism can affect drug clearance, efficacy, and toxicity. Inhibitors or inducers of these enzymes can lead to significant drug interactions, altering the expected plasma concentrations of medications.\n",
    "\n",
    "#### Question: How does CRISPR-Cas9 technology enable genome editing?\n",
    "#### Answer: \n",
    "CRISPR-Cas9 provides the ability to target specific regions of a genome from a patient's immune cells and deliver targeted DNA to cells outside the body. The development of new therapies targeting specific regions of cells leads to the development of novel therapies, which in turn leads to new therapies. In addition, CRISPR-Cas9 creates functional immune cells that can recognize and respond to different types of pathogens, which in turn leads to novel therapeutics.\n",
    "#### Expected Answer:\n",
    "CRISPR-Cas9 is a revolutionary genome editing tool that enables precise modifications to DNA. It uses a short guide RNA (gRNA) to direct the Cas9 nuclease to a specific sequence in the genome. Once there, Cas9 introduces a double-stranded break in the DNA. The cellâ€™s natural repair mechanisms then fix the break, either by non-homologous end joining (NHEJ), which can disrupt gene function, or by homology-directed repair (HDR) if a repair template is provided, allowing for precise changes. This targeted editing allows scientists to knock out genes, correct mutations, or insert new genetic material, facilitating advances in both research and potential therapeutic applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec365f-a168-463a-9475-8526ee4c4333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
